

# ğŸš€ End-to-End MLOps Project â€“ From Data to Deployment

An **industrial-grade Machine Learning pipeline** that takes raw data from a live database, processes it through a robust CI/CD-enabled MLOps workflow, trains and validates a model, and deploys it on AWS using Docker & FastAPI â€” fully automated with GitHub Actions.

![MLOps Flow](assets/mlops_pipeline.png) <!-- optional: insert a pipeline diagram -->

---

## ğŸ“Œ Features

* **Automated Project Structure Generation** â€“ Uses `template.py` to scaffold folders/files instantly.
* **Environment Isolation** â€“ Reproducible builds using Conda environments & `requirements.txt`.
* **Cloud-Hosted NoSQL Database** â€“ MongoDB Atlas for real-time data storage & retrieval.
* **Modular Components** â€“ Data Ingestion, Validation, Transformation, Training, and Model Pushing.
* **Model Versioning & Storage** â€“ AWS S3 integration for storing and retrieving models.
* **FastAPI Deployment** â€“ Model served via a REST API with async support.
* **Containerized & Cloud-Ready** â€“ Dockerized for portability, deployed on AWS EC2.
* **CI/CD Pipeline** â€“ Automated testing, build, and deployment with GitHub Actions.

---

## ğŸ—ï¸ Pipeline Overview

| Stage                      | Description                                                                                |
| -------------------------- | ------------------------------------------------------------------------------------------ |
| **1. Project Setup**       | Auto-generate folders & files with `template.py`; configure `setup.py` & `pyproject.toml`. |
| **2. Environment Setup**   | Create & activate a dedicated Conda environment; install dependencies.                     |
| **3. Database Setup**      | Connect to MongoDB Atlas; store & retrieve datasets securely via environment variables.    |
| **4. Data Ingestion**      | Pull raw data from MongoDB, split into train/test, store artifacts.                        |
| **5. Data Validation**     | Validate schema & columns using `schema.yaml`.                                             |
| **6. Data Transformation** | Scale & preprocess data for training.                                                      |
| **7. Model Training**      | Train ML model with predefined hyperparameters; save if performance improves.              |
| **8. Model Storage**       | Push trained model to AWS S3 for versioning.                                               |
| **9. Model Deployment**    | Deploy via FastAPI; containerize with Docker.                                              |
| **10. CI/CD Automation**   | GitHub Actions pipeline for build, push, and deploy to AWS EC2.                            |

---

## ğŸ› ï¸ Tech Stack

**Languages & Frameworks:**

* Python, FastAPI, Pandas, Scikit-learn, PyMongo

**Cloud & Infrastructure:**

* MongoDB Atlas, AWS S3, AWS ECR, AWS EC2

**DevOps & Automation:**

* Docker, GitHub Actions, CI/CD, Environment Variables

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/           # Modular pipeline components
â”‚   â”œâ”€â”€ entity/               # Config & artifact entities
â”‚   â”œâ”€â”€ utils/                # Helper functions
â”‚   â”œâ”€â”€ constants/            # Global constants
â”‚   â”œâ”€â”€ pipelines/            # Training & prediction pipelines
â”‚   â””â”€â”€ config/               # DB & cloud configurations
â”œâ”€â”€ artifacts/                # Generated datasets & reports (gitignored)
â”œâ”€â”€ .github/workflows/        # CI/CD workflows
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## âš¡ Quick Start

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/mlops-project.git
cd mlops-project
```

### 2ï¸âƒ£ Create and activate Conda environment

```bash
conda create -n mlops-env python=3.10 -y
conda activate mlops-env
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set up environment variables

```bash
export MONGODB_URL="your_mongo_connection_string"
export AWS_ACCESS_KEY_ID="your_aws_key"
export AWS_SECRET_ACCESS_KEY="your_aws_secret"
```

### 5ï¸âƒ£ Run training pipeline

```bash
python src/pipelines/training_pipeline.py
```

### 6ï¸âƒ£ Deploy with FastAPI

```bash
uvicorn app:app --reload
```

---

## ğŸŒ Deployment

* **Docker** â€“ Build and run locally:

  ```bash
  docker build -t mlops-app .
  docker run -p 5000:5000 mlops-app
  ```
* **AWS EC2** â€“ Pull Docker image from AWS ECR & run.
* **CI/CD** â€“ Triggered on `git push` to `main`, fully automating build & deploy.

---

## ğŸ“Š Key Highlights for Recruiters

âœ… **End-to-End Ownership** â€“ Covers *data ingestion â†’ deployment*
âœ… **Production-Grade MLOps** â€“ Environment variables, logging, exception handling, artifacts
âœ… **Cloud-Native** â€“ MongoDB Atlas + AWS S3 + AWS EC2 + AWS ECR
âœ… **Automation First** â€“ GitHub Actions CI/CD pipeline
âœ… **Scalable API** â€“ FastAPI with async support

---

## ğŸ§  Future Enhancements

* Add monitoring with Prometheus & Grafana
* Enable A/B testing for model selection
* Integrate MLflow for experiment tracking

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, open an issue first to discuss.

---

## ğŸ“œ License

MIT License. See `LICENSE` for details.

